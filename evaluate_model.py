#!/usr/bin/env python
"""
æ¨¡å‹è¯„ä¼°è„šæœ¬ - æµ‹è¯•å¾®è°ƒåçš„å·¥å…·è°ƒç”¨èƒ½åŠ›
"""

import json
import torch
from pathlib import Path


def load_model(model_path: str):
    """åŠ è½½å¾®è°ƒåçš„æ¨¡å‹"""
    try:
        from unsloth import FastLanguageModel
        print(f"ä½¿ç”¨ Unsloth åŠ è½½æ¨¡å‹: {model_path}")
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name=model_path,
            max_seq_length=2048,
            dtype=None,
            load_in_4bit=True,
        )
        FastLanguageModel.for_inference(model)
        return model, tokenizer
    except ImportError:
        from transformers import AutoModelForCausalLM, AutoTokenizer
        from peft import PeftModel
        
        print(f"åŠ è½½æ¨¡å‹: {model_path}")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ LoRA adapter
        adapter_config = Path(model_path) / "adapter_config.json"
        if adapter_config.exists():
            print("æ£€æµ‹åˆ° LoRA adapterï¼ŒåŠ è½½åŸºç¡€æ¨¡å‹...")
            base_model = AutoModelForCausalLM.from_pretrained(
                "Qwen/Qwen2.5-3B-Instruct",
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True,
            )
            model = PeftModel.from_pretrained(base_model, model_path)
        else:
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True,
            )
        
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        return model, tokenizer


def generate_response(model, tokenizer, messages: list, max_new_tokens: int = 512):
    """ç”Ÿæˆæ¨¡å‹å“åº”"""
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )
    
    inputs = tokenizer(text, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.1,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    response = tokenizer.decode(outputs[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True)
    return response


def evaluate_tool_calling(model, tokenizer):
    """è¯„ä¼°å·¥å…·è°ƒç”¨èƒ½åŠ›"""
    
    # ç³»ç»Ÿæç¤ºè¯
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªè§†é¢‘å¤„ç†åŠ©æ‰‹ã€‚ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·:

1. parse_video(video_url) - è§£æè§†é¢‘ä¿¡æ¯
2. generate_subtitles(video_url, source_language) - ç”Ÿæˆå­—å¹•
3. translate_subtitles(subtitle_id, target_language) - ç¿»è¯‘å­—å¹•
4. add_dubbing(video_url, voice_style, target_language) - æ·»åŠ é…éŸ³
5. download_file(file_id, format) - ä¸‹è½½æ–‡ä»¶

å½“éœ€è¦ä½¿ç”¨å·¥å…·æ—¶ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹ JSON æ ¼å¼:
{"tool": "å·¥å…·å", "params": {"å‚æ•°å": "å‚æ•°å€¼"}}"""

    # æµ‹è¯•ç”¨ä¾‹ - 20ä¸ªï¼Œä»ç®€å•åˆ°å¤æ‚é€’å¢
    # expected_params: æœŸæœ›æå–çš„å‚æ•°åŠå…¶å€¼ï¼ˆç”¨äºéªŒè¯å‚æ•°æå–èƒ½åŠ›ï¼‰
    test_cases = [
        # ============ ç®€å•çº§åˆ« (1-5) - å•å·¥å…·ã€ç›´æ¥æŒ‡ä»¤ ============
        {
            "query": "è§£æè§†é¢‘ https://example.com/video.mp4",
            "expected_tools": ["parse_video"],
            "expected_params": {"video_url": "https://example.com/video.mp4"},
            "difficulty": "ç®€å•",
            "description": "æœ€åŸºç¡€çš„å•å·¥å…·è°ƒç”¨",
        },
        {
            "query": "ä¸‹è½½ file_001ï¼Œmp4æ ¼å¼",
            "expected_tools": ["download_file"],
            "expected_params": {"file_id": "file_001", "format": "mp4"},
            "difficulty": "ç®€å•",
            "description": "ç®€å•ä¸‹è½½è¯·æ±‚",
        },
        {
            "query": "ç»™è§†é¢‘åŠ å­—å¹• https://test.com/movie.mp4 è¯­è¨€æ˜¯è‹±è¯­",
            "expected_tools": ["generate_subtitles"],
            "expected_params": {"video_url": "https://test.com/movie.mp4", "source_language": "è‹±è¯­"},
            "difficulty": "ç®€å•",
            "description": "ç”Ÿæˆå­—å¹•åŸºç¡€è°ƒç”¨",
        },
        {
            "query": "ç¿»è¯‘å­—å¹• sub_123 åˆ°æ—¥è¯­",
            "expected_tools": ["translate_subtitles"],
            "expected_params": {"subtitle_id": "sub_123", "target_language": "æ—¥è¯­"},
            "difficulty": "ç®€å•",
            "description": "å­—å¹•ç¿»è¯‘åŸºç¡€è°ƒç”¨",
        },
        {
            "query": "ç»™ https://example.com/clip.mp4 é…éŸ³ï¼Œç”¨å¥³å£°ï¼Œç›®æ ‡è¯­è¨€ä¸­æ–‡",
            "expected_tools": ["add_dubbing"],
            "expected_params": {"video_url": "https://example.com/clip.mp4", "voice_style": "å¥³å£°", "target_language": "ä¸­æ–‡"},
            "difficulty": "ç®€å•",
            "description": "é…éŸ³åŸºç¡€è°ƒç”¨",
        },
        
        # ============ ä¸­ç­‰çº§åˆ« (6-10) - å•å·¥å…·ã€å£è¯­åŒ–è¡¨è¾¾ ============
        {
            "query": "æˆ‘æœ‰ä¸ªYouTubeè§†é¢‘æƒ³çœ‹çœ‹é‡Œé¢æœ‰ä»€ä¹ˆå†…å®¹ï¼Œé“¾æ¥æ˜¯ https://youtube.com/watch?v=abc123",
            "expected_tools": ["parse_video"],
            "expected_params": {"video_url": "https://youtube.com/watch?v=abc123"},
            "difficulty": "ä¸­ç­‰",
            "description": "å£è¯­åŒ–è¡¨è¾¾è§£æéœ€æ±‚",
        },
        {
            "query": "èƒ½å¸®æˆ‘æŠŠè¿™ä¸ªæ—¥æœ¬åŠ¨æ¼«çš„å­—å¹•ç¿»è¯‘æˆä¸­æ–‡å—ï¼Ÿå­—å¹•æ–‡ä»¶ç¼–å·æ˜¯ sub_anime_456",
            "expected_tools": ["translate_subtitles"],
            "expected_params": {"subtitle_id": "sub_anime_456", "target_language": "ä¸­æ–‡"},
            "difficulty": "ä¸­ç­‰",
            "description": "å¸¦åœºæ™¯æè¿°çš„ç¿»è¯‘è¯·æ±‚",
        },
        {
            "query": "æˆ‘ä¸‹è½½äº†ä¸€ä¸ªéŸ©å‰§è§†é¢‘ https://drama.com/ep01.mp4ï¼Œæƒ³ç»™å®ƒè‡ªåŠ¨ç”ŸæˆéŸ©è¯­å­—å¹•",
            "expected_tools": ["generate_subtitles"],
            "expected_params": {"video_url": "https://drama.com/ep01.mp4", "source_language": "éŸ©è¯­"},
            "difficulty": "ä¸­ç­‰",
            "description": "å¸¦èƒŒæ™¯è¯´æ˜çš„å­—å¹•ç”Ÿæˆ",
        },
        {
            "query": "å¤„ç†å®Œçš„è§†é¢‘æˆ‘æƒ³å¯¼å‡ºæ¥ï¼Œæ–‡ä»¶IDæ˜¯ processed_789ï¼Œè¦é«˜æ¸…mp4",
            "expected_tools": ["download_file"],
            "expected_params": {"file_id": "processed_789", "format": "mp4"},
            "difficulty": "ä¸­ç­‰",
            "description": "å£è¯­åŒ–ä¸‹è½½è¯·æ±‚",
        },
        {
            "query": "è¿™ä¸ªè‹±è¯­æ•™å­¦è§†é¢‘éœ€è¦é…ä¸Šæ ‡å‡†ç¾å¼å‘éŸ³çš„è‹±è¯­æ—ç™½ https://edu.com/lesson1.mp4",
            "expected_tools": ["add_dubbing"],
            "expected_params": {"video_url": "https://edu.com/lesson1.mp4", "target_language": "è‹±è¯­"},
            "difficulty": "ä¸­ç­‰",
            "description": "å¸¦å…·ä½“è¦æ±‚çš„é…éŸ³è¯·æ±‚",
        },
        
        # ============ è¾ƒéš¾çº§åˆ« (11-15) - å¤šæ­¥éª¤æš—ç¤ºã€å¤æ‚åœºæ™¯ ============
        {
            "query": "æˆ‘æ˜¯ä¸ªUPä¸»ï¼Œåˆšå½•äº†ä¸ªæ¸¸æˆè§£è¯´è§†é¢‘ https://bilibili.com/video/BV123ï¼Œæƒ³å…ˆçœ‹çœ‹è§†é¢‘æ—¶é•¿å’Œåˆ†è¾¨ç‡ä¿¡æ¯",
            "expected_tools": ["parse_video"],
            "expected_params": {"video_url": "https://bilibili.com/video/BV123"},
            "difficulty": "è¾ƒéš¾",
            "description": "å¸¦èº«ä»½å’Œåœºæ™¯çš„è§£æè¯·æ±‚",
        },
        {
            "query": "å…¬å¸è¦åšä¸€ä¸ªäº§å“å®£ä¼ ç‰‡çš„å¤šè¯­è¨€ç‰ˆæœ¬ï¼ŒåŸç‰‡æ˜¯ https://company.com/promo.mp4ï¼Œå…ˆå¸®æˆ‘è¯†åˆ«å‡ºä¸­æ–‡å­—å¹•",
            "expected_tools": ["generate_subtitles"],
            "expected_params": {"video_url": "https://company.com/promo.mp4", "source_language": "ä¸­æ–‡"},
            "difficulty": "è¾ƒéš¾",
            "description": "ä¼ä¸šåœºæ™¯çš„å­—å¹•ç”Ÿæˆ",
        },
        {
            "query": "æˆ‘ä»¬å›¢é˜Ÿç¿»è¯‘å¥½äº†ä¸€ä»½è¥¿ç­ç‰™è¯­å­—å¹• sub_spanish_docï¼Œç°åœ¨éœ€è¦è½¬æˆè‘¡è„ç‰™è¯­ç»™å·´è¥¿åˆ†å…¬å¸ç”¨",
            "expected_tools": ["translate_subtitles"],
            "expected_params": {"subtitle_id": "sub_spanish_doc", "target_language": "è‘¡è„ç‰™è¯­"},
            "difficulty": "è¾ƒéš¾",
            "description": "è·¨å›½ä¸šåŠ¡åœºæ™¯çš„ç¿»è¯‘",
        },
        {
            "query": "æˆ‘åœ¨åšä¸€ä¸ªé¢å‘ä¸œå—äºšå¸‚åœºçš„APPä»‹ç»è§†é¢‘ https://app.com/intro.mp4ï¼Œéœ€è¦æ³°è¯­é…éŸ³ï¼Œå£°éŸ³è¦å¹´è½»æ´»æ³¼çš„å¥³å£°é£æ ¼",
            "expected_tools": ["add_dubbing"],
            "expected_params": {"video_url": "https://app.com/intro.mp4", "target_language": "æ³°è¯­", "voice_style": "å¥³å£°"},
            "difficulty": "è¾ƒéš¾",
            "description": "è¯¦ç»†è¦æ±‚çš„é…éŸ³åœºæ™¯",
        },
        {
            "query": "å®¢æˆ·å‚¬ç€è¦æœ€ç»ˆç‰ˆè§†é¢‘äº†ï¼Œæ–‡ä»¶ç¼–å· final_cut_2024ï¼Œå¯¼å‡ºæˆmovæ ¼å¼æ–¹ä¾¿ä»–ä»¬åœ¨Macä¸Šç¼–è¾‘",
            "expected_tools": ["download_file"],
            "expected_params": {"file_id": "final_cut_2024", "format": "mov"},
            "difficulty": "è¾ƒéš¾",
            "description": "ç´§æ€¥ä¸šåŠ¡åœºæ™¯çš„ä¸‹è½½",
        },
        
        # ============ å¤æ‚çº§åˆ« (16-18) - å¤šå·¥å…·ä¸²è”æš—ç¤º ============
        {
            "query": "æˆ‘æœ‰ä¸€ä¸ªè‹±æ–‡çš„TEDæ¼”è®²è§†é¢‘ https://ted.com/talk123.mp4ï¼Œæƒ³åšæˆä¸­æ–‡ç‰ˆå‘åˆ°Bç«™ã€‚é¦–å…ˆå¸®æˆ‘åˆ†æä¸€ä¸‹è¿™ä¸ªè§†é¢‘çš„åŸºæœ¬ä¿¡æ¯",
            "expected_tools": ["parse_video"],
            "expected_params": {"video_url": "https://ted.com/talk123.mp4"},
            "difficulty": "å¤æ‚",
            "description": "å¤šæ­¥éª¤ä»»åŠ¡çš„ç¬¬ä¸€æ­¥",
        },
        {
            "query": "ç»§ç»­ä¸Šä¸ªä»»åŠ¡ï¼Œè§†é¢‘åˆ†æå®Œäº†ï¼Œç°åœ¨éœ€è¦å…ˆæå–å‡ºè‹±æ–‡å­—å¹•ï¼Œè§†é¢‘åœ°å€è¿˜æ˜¯ https://ted.com/talk123.mp4",
            "expected_tools": ["generate_subtitles"],
            "expected_params": {"video_url": "https://ted.com/talk123.mp4", "source_language": "è‹±æ–‡"},
            "difficulty": "å¤æ‚",
            "description": "å¤šæ­¥éª¤ä»»åŠ¡çš„ä¸­é—´æ­¥éª¤",
        },
        {
            "query": "å­—å¹•æå–å¥½äº†ï¼Œç¼–å·æ˜¯ sub_ted_en_001ï¼Œè¯·æŠŠå®ƒç¿»è¯‘æˆç®€ä½“ä¸­æ–‡ï¼Œæˆ‘è¦ç”¨æ¥åšåŒè¯­å­—å¹•",
            "expected_tools": ["translate_subtitles"],
            "expected_params": {"subtitle_id": "sub_ted_en_001", "target_language": "ä¸­æ–‡"},
            "difficulty": "å¤æ‚",
            "description": "å¤šæ­¥éª¤ä»»åŠ¡çš„åç»­æ­¥éª¤",
        },
        
        # ============ éå¸¸å¤æ‚çº§åˆ« (19-20) - å¤šå·¥å…·ã€å¤æ‚åœºæ™¯ã€å¤šæ¡ä»¶ ============
        {
            "query": "æˆ‘æ˜¯ä¸€ä¸ªè‡ªåª’ä½“åšä¸»ï¼Œæœ€è¿‘æ¥äº†ä¸ªè·¨å¢ƒç”µå•†çš„æ¨å¹¿å•ã€‚å®¢æˆ·ç»™äº†ä¸ªè‹±æ–‡äº§å“ä»‹ç»è§†é¢‘ https://amazon.com/product_demo.mp4ï¼Œæˆ‘éœ€è¦ï¼š1ï¼‰å…ˆåˆ†æè§†é¢‘äº†è§£å†…å®¹ç»“æ„ï¼›2ï¼‰ç„¶åæå–è‹±æ–‡å­—å¹•ã€‚è§†é¢‘å¤§æ¦‚3åˆ†é’Ÿï¼Œäº§å“æ˜¯æ™ºèƒ½æ‰‹è¡¨ã€‚å…ˆå¸®æˆ‘åšç¬¬ä¸€æ­¥ï¼Œè§£æè§†é¢‘ä¿¡æ¯",
            "expected_tools": ["parse_video"],
            "expected_params": {"video_url": "https://amazon.com/product_demo.mp4"},
            "difficulty": "éå¸¸å¤æ‚",
            "description": "å®Œæ•´ä¸šåŠ¡åœºæ™¯+å¤šæ­¥éª¤è§„åˆ’+å…·ä½“äº§å“æè¿°",
        },
        {
            "query": "æˆ‘ä»¬æ˜¯ä¸€å®¶MCNæœºæ„ï¼Œæ­£åœ¨å¸®ä¸€ä¸ªæ—¥æœ¬ç¾å¦†åšä¸»åšä¸­å›½å¸‚åœºæœ¬åœ°åŒ–ã€‚å¥¹çš„æœ€æ–°è§†é¢‘ https://youtube.com/beauty_tips.mp4 éœ€è¦å®Œæ•´å¤„ç†ï¼šç›®å‰è§†é¢‘æ˜¯æ—¥è¯­çš„ï¼Œæˆ‘ä»¬å·²ç»äººå·¥ç¿»è¯‘å¥½äº†ä¸­æ–‡å­—å¹•æ–‡ä»¶ sub_beauty_cn_finalï¼Œç°åœ¨éœ€è¦æ‰¾ä¸€ä¸ªç”œç¾é£æ ¼çš„ä¸­æ–‡å¥³å£°æ¥é…éŸ³ï¼Œè®©ä¸­å›½è§‚ä¼—å¬èµ·æ¥æ›´äº²åˆ‡è‡ªç„¶ã€‚è¯·å¸®æˆ‘æ·»åŠ é…éŸ³",
            "expected_tools": ["add_dubbing"],
            "expected_params": {"video_url": "https://youtube.com/beauty_tips.mp4", "target_language": "ä¸­æ–‡", "voice_style": "å¥³å£°"},
            "difficulty": "éå¸¸å¤æ‚",
            "description": "MCNä¸šåŠ¡åœºæ™¯+è·¨å›½æœ¬åœ°åŒ–+è¯¦ç»†å£°éŸ³è¦æ±‚+å®Œæ•´èƒŒæ™¯è¯´æ˜",
        },
    ]
    
    print("\n" + "=" * 70)
    print("å·¥å…·è°ƒç”¨èƒ½åŠ›è¯„ä¼° - 20ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼ˆç®€å•â†’éå¸¸å¤æ‚ï¼‰")
    print("=" * 70)
    
    results = {
        "ç®€å•": {"tool_correct": 0, "param_correct": 0, "total": 0},
        "ä¸­ç­‰": {"tool_correct": 0, "param_correct": 0, "total": 0},
        "è¾ƒéš¾": {"tool_correct": 0, "param_correct": 0, "total": 0},
        "å¤æ‚": {"tool_correct": 0, "param_correct": 0, "total": 0},
        "éå¸¸å¤æ‚": {"tool_correct": 0, "param_correct": 0, "total": 0},
    }
    
    total = len(test_cases)
    total_tool_score = 0
    total_param_score = 0
    
    for i, case in enumerate(test_cases, 1):
        difficulty = case["difficulty"]
        results[difficulty]["total"] += 1
        
        print(f"\n{'='*70}")
        print(f"[æµ‹è¯• {i}/{total}] éš¾åº¦: {difficulty}")
        print(f"åœºæ™¯: {case['description']}")
        print(f"{'='*70}")
        print(f"è¾“å…¥: {case['query'][:100]}..." if len(case['query']) > 100 else f"è¾“å…¥: {case['query']}")
        print(f"æœŸæœ›å·¥å…·: {case['expected_tools']}")
        print(f"æœŸæœ›å‚æ•°: {case['expected_params']}")
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": case["query"]},
        ]
        
        response = generate_response(model, tokenizer, messages)
        print(f"\nè¾“å‡º: {response[:400]}..." if len(response) > 400 else f"\nè¾“å‡º: {response}")
        
        # è¯„ä¼°ç»“æœ
        expected_tools = case["expected_tools"]
        expected_params = case["expected_params"]
        
        tool_score = 0
        param_score = 0
        found_tool = None
        found_params = {}
        
        # å°è¯•è§£æ JSON
        try:
            start = response.find("{")
            end = response.rfind("}") + 1
            if start != -1 and end > start:
                json_str = response[start:end]
                parsed = json.loads(json_str)
                
                found_tool = parsed.get("tool")
                found_params = parsed.get("params", {})
                
                # 1. æ£€æŸ¥å·¥å…·æ˜¯å¦æ­£ç¡®
                if found_tool in expected_tools:
                    tool_score = 1
                    print(f"\nâœ… å·¥å…·æ­£ç¡®: {found_tool}")
                else:
                    print(f"\nâŒ å·¥å…·é”™è¯¯: æœŸæœ› {expected_tools}, å¾—åˆ° {found_tool}")
                
                # 2. æ£€æŸ¥å‚æ•°æå–
                if found_params:
                    print(f"\nå‚æ•°æå–æ£€æŸ¥:")
                    matched_params = 0
                    total_expected = len(expected_params)
                    
                    for param_name, expected_value in expected_params.items():
                        actual_value = found_params.get(param_name)
                        
                        if actual_value is None:
                            print(f"  âŒ {param_name}: æœªæå– (æœŸæœ›: {expected_value})")
                        else:
                            # æ£€æŸ¥å€¼æ˜¯å¦åŒ¹é…ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰
                            if check_param_match(expected_value, actual_value):
                                print(f"  âœ… {param_name}: {actual_value}")
                                matched_params += 1
                            else:
                                print(f"  âš ï¸ {param_name}: {actual_value} (æœŸæœ›: {expected_value})")
                                matched_params += 0.5  # éƒ¨åˆ†åŒ¹é…
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰å¤šä½™å‚æ•°
                    extra_params = set(found_params.keys()) - set(expected_params.keys())
                    if extra_params:
                        print(f"  â„¹ï¸ é¢å¤–å‚æ•°: {extra_params}")
                    
                    param_score = matched_params / total_expected if total_expected > 0 else 0
                    print(f"\n  å‚æ•°å¾—åˆ†: {matched_params}/{total_expected} ({param_score*100:.0f}%)")
                else:
                    print(f"\nâŒ æœªæå–åˆ°ä»»ä½•å‚æ•°")
                    
        except json.JSONDecodeError as e:
            # JSON è§£æå¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«å·¥å…·å
            for tool in expected_tools:
                if tool in response:
                    print(f"\nâš ï¸ åŒ…å«å·¥å…·å '{tool}' ä½†JSONæ ¼å¼ä¸æ ‡å‡†")
                    tool_score = 0.5
                    break
            else:
                print(f"\nâŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„å·¥å…·è°ƒç”¨")
        
        # æ±‡æ€»æœ¬æ¡å¾—åˆ†
        total_tool_score += tool_score
        total_param_score += param_score
        results[difficulty]["tool_correct"] += tool_score
        results[difficulty]["param_correct"] += param_score
        
        # ç»¼åˆè¯„ä»·
        if tool_score == 1 and param_score >= 0.8:
            print(f"\nğŸ¯ ç»¼åˆè¯„ä»·: ä¼˜ç§€")
        elif tool_score >= 0.5 and param_score >= 0.5:
            print(f"\nğŸ‘ ç»¼åˆè¯„ä»·: è‰¯å¥½")
        elif tool_score >= 0.5 or param_score >= 0.3:
            print(f"\nâš ï¸ ç»¼åˆè¯„ä»·: ä¸€èˆ¬")
        else:
            print(f"\nâŒ ç»¼åˆè¯„ä»·: è¾ƒå·®")
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 70)
    print("è¯„ä¼°ç»“æœæ±‡æ€»")
    print("=" * 70)
    
    # æŒ‰éš¾åº¦æ˜¾ç¤ºç»“æœ
    print("\næŒ‰éš¾åº¦åˆ†ç±» (å·¥å…·è¯†åˆ« / å‚æ•°æå–):")
    print("-" * 60)
    for difficulty in ["ç®€å•", "ä¸­ç­‰", "è¾ƒéš¾", "å¤æ‚", "éå¸¸å¤æ‚"]:
        r = results[difficulty]
        if r["total"] > 0:
            tool_acc = r["tool_correct"] / r["total"] * 100
            param_acc = r["param_correct"] / r["total"] * 100
            tool_bar = "â–ˆ" * int(tool_acc / 10) + "â–‘" * (10 - int(tool_acc / 10))
            param_bar = "â–ˆ" * int(param_acc / 10) + "â–‘" * (10 - int(param_acc / 10))
            print(f"  {difficulty:8s}:")
            print(f"    å·¥å…·: {r['tool_correct']:4.1f}/{r['total']} ({tool_acc:5.1f}%) {tool_bar}")
            print(f"    å‚æ•°: {r['param_correct']:4.1f}/{r['total']} ({param_acc:5.1f}%) {param_bar}")
    
    # æ€»ä½“ç»“æœ
    tool_accuracy = total_tool_score / total * 100
    param_accuracy = total_param_score / total * 100
    overall_accuracy = (tool_accuracy + param_accuracy) / 2
    
    print("-" * 60)
    print(f"  {'æ€»è®¡':8s}:")
    print(f"    å·¥å…·è¯†åˆ«: {total_tool_score:4.1f}/{total} ({tool_accuracy:5.1f}%)")
    print(f"    å‚æ•°æå–: {total_param_score:4.1f}/{total} ({param_accuracy:5.1f}%)")
    print(f"    ç»¼åˆå¾—åˆ†: {overall_accuracy:.1f}%")
    print()
    
    # è¯„ä»·
    if overall_accuracy >= 85:
        print("ğŸ‰ æ¨¡å‹è¡¨ç°ä¼˜ç§€! å·¥å…·è°ƒç”¨å’Œå‚æ•°æå–èƒ½åŠ›éƒ½å¾ˆå¼º")
    elif overall_accuracy >= 70:
        print("ğŸ‘ æ¨¡å‹è¡¨ç°è‰¯å¥½ï¼Œå¯ç»§ç»­ä¼˜åŒ–å‚æ•°æå–å‡†ç¡®åº¦")
    elif overall_accuracy >= 50:
        print("âš ï¸ æ¨¡å‹è¡¨ç°ä¸€èˆ¬ï¼Œå»ºè®®å¢åŠ è®­ç»ƒæ•°æ®")
    else:
        print("âŒ æ¨¡å‹è¡¨ç°è¾ƒå·®ï¼Œéœ€è¦æ›´å¤šè®­ç»ƒæ•°æ®å’Œä¼˜åŒ–")
    
    # ç»™å‡ºæ”¹è¿›å»ºè®®
    print("\næ”¹è¿›å»ºè®®:")
    
    # åˆ†æå·¥å…·è¯†åˆ«å¼±é¡¹
    weak_tool_areas = []
    weak_param_areas = []
    for difficulty in ["ç®€å•", "ä¸­ç­‰", "è¾ƒéš¾", "å¤æ‚", "éå¸¸å¤æ‚"]:
        r = results[difficulty]
        if r["total"] > 0:
            if r["tool_correct"] / r["total"] < 0.6:
                weak_tool_areas.append(difficulty)
            if r["param_correct"] / r["total"] < 0.6:
                weak_param_areas.append(difficulty)
    
    if weak_tool_areas:
        print(f"  - å·¥å…·è¯†åˆ«åœ¨ {', '.join(weak_tool_areas)} çº§åˆ«è¾ƒå¼±")
    if weak_param_areas:
        print(f"  - å‚æ•°æå–åœ¨ {', '.join(weak_param_areas)} çº§åˆ«è¾ƒå¼±")
    
    if param_accuracy < tool_accuracy - 10:
        print("  - å‚æ•°æå–èƒ½åŠ›æ˜æ˜¾å¼±äºå·¥å…·è¯†åˆ«ï¼Œå»ºè®®å¢åŠ å‚æ•°æå–çš„è®­ç»ƒæ ·æœ¬")
    
    if results["éå¸¸å¤æ‚"]["total"] > 0:
        complex_score = (results["éå¸¸å¤æ‚"]["tool_correct"] + results["éå¸¸å¤æ‚"]["param_correct"]) / (results["éå¸¸å¤æ‚"]["total"] * 2)
        if complex_score < 0.5:
            print("  - å¤æ‚åœºæ™¯ç†è§£èƒ½åŠ›ä¸è¶³ï¼Œå»ºè®®å¢åŠ å¸¦è¯¦ç»†åœºæ™¯æè¿°çš„è®­ç»ƒæ•°æ®")
    
    return overall_accuracy


def check_param_match(expected: str, actual: str) -> bool:
    """
    æ£€æŸ¥å‚æ•°å€¼æ˜¯å¦åŒ¹é…ï¼ˆæ”¯æŒæ¨¡ç³ŠåŒ¹é…ï¼‰
    """
    if expected is None or actual is None:
        return False
    
    expected_str = str(expected).lower().strip()
    actual_str = str(actual).lower().strip()
    
    # å®Œå…¨åŒ¹é…
    if expected_str == actual_str:
        return True
    
    # URL åŒ¹é…ï¼ˆå¿½ç•¥åè®®å·®å¼‚ï¼‰
    if expected_str.startswith("http"):
        expected_clean = expected_str.replace("https://", "").replace("http://", "")
        actual_clean = actual_str.replace("https://", "").replace("http://", "")
        if expected_clean == actual_clean:
            return True
    
    # è¯­è¨€åç§°æ¨¡ç³ŠåŒ¹é…
    language_aliases = {
        "ä¸­æ–‡": ["ä¸­æ–‡", "chinese", "zh", "cn", "ç®€ä½“ä¸­æ–‡", "ä¸­å›½è¯­"],
        "è‹±è¯­": ["è‹±è¯­", "english", "en", "è‹±æ–‡"],
        "æ—¥è¯­": ["æ—¥è¯­", "japanese", "ja", "jp", "æ—¥æ–‡"],
        "éŸ©è¯­": ["éŸ©è¯­", "korean", "ko", "kr", "éŸ©æ–‡"],
        "æ³°è¯­": ["æ³°è¯­", "thai", "th"],
        "è‘¡è„ç‰™è¯­": ["è‘¡è„ç‰™è¯­", "portuguese", "pt", "è‘¡è¯­"],
        "è¥¿ç­ç‰™è¯­": ["è¥¿ç­ç‰™è¯­", "spanish", "es"],
    }
    
    for lang, aliases in language_aliases.items():
        if expected_str in [a.lower() for a in aliases]:
            if actual_str in [a.lower() for a in aliases]:
                return True
    
    # åŒ…å«åŒ¹é…ï¼ˆå®é™…å€¼åŒ…å«æœŸæœ›å€¼çš„å…³é”®éƒ¨åˆ†ï¼‰
    if expected_str in actual_str or actual_str in expected_str:
        return True
    
    return False


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="è¯„ä¼°å¾®è°ƒæ¨¡å‹")
    parser.add_argument("--model-path", type=str, default="outputs/tool_model", help="æ¨¡å‹è·¯å¾„")
    args = parser.parse_args()
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    model_path = Path(args.model_path)
    if not model_path.exists():
        # å°è¯•åˆå¹¶æ¨¡å‹è·¯å¾„
        merged_path = Path(f"{args.model_path}_merged")
        if merged_path.exists():
            model_path = merged_path
            print(f"ä½¿ç”¨åˆå¹¶æ¨¡å‹: {model_path}")
        else:
            print(f"é”™è¯¯: æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.model_path}")
            print("è¯·å…ˆè¿è¡Œè®­ç»ƒ: python generate_and_train.py")
            return
    
    print(f"æ¨¡å‹è·¯å¾„: {model_path}")
    
    # åŠ è½½æ¨¡å‹
    model, tokenizer = load_model(str(model_path))
    
    # è¯„ä¼°
    accuracy = evaluate_tool_calling(model, tokenizer)
    
    print(f"\næœ€ç»ˆå‡†ç¡®ç‡: {accuracy:.1f}%")


if __name__ == "__main__":
    main()
