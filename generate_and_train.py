#!/usr/bin/env python
"""
ç”Ÿæˆè®­ç»ƒæ•°æ®å¹¶ä½¿ç”¨ GPU è®­ç»ƒ
æ— éœ€ openai/datasets ç­‰å¤–éƒ¨ä¾èµ–

ä½¿ç”¨æ–¹æ³•:
    python generate_and_train.py --samples 500 --epochs 3
"""

import asyncio
import json
import random
import argparse
import sys
from pathlib import Path
from dataclasses import dataclass
from enum import Enum

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))


# ============================================================================
# æ•°æ®ç”Ÿæˆéƒ¨åˆ† - ä¸ä¾èµ–å¤–éƒ¨ API
# ============================================================================

class ToolCategory(str, Enum):
    VIDEO_PROCESSING = "video_processing"
    SUBTITLE_GENERATION = "subtitle_generation"
    AUDIO_DUBBING = "audio_dubbing"
    FILE_MANAGEMENT = "file_management"
    TRANSLATION = "translation"
    CONTENT_ANALYSIS = "content_analysis"
    SCHEDULING = "scheduling"


# å·¥å…·å®šä¹‰
TOOLS = [
    {
        "name": "parse_video",
        "category": ToolCategory.VIDEO_PROCESSING,
        "description": "è§£æè§†é¢‘ä¿¡æ¯",
        "params": {"video_url": "string"},
        "required": ["video_url"]
    },
    {
        "name": "generate_subtitles",
        "category": ToolCategory.SUBTITLE_GENERATION,
        "description": "ç”Ÿæˆå­—å¹•",
        "params": {"video_url": "string", "source_language": "string"},
        "required": ["video_url"]
    },
    {
        "name": "translate_subtitles",
        "category": ToolCategory.TRANSLATION,
        "description": "ç¿»è¯‘å­—å¹•",
        "params": {"subtitle_file": "string", "source_language": "string", "target_language": "string"},
        "required": ["subtitle_file", "target_language"]
    },
    {
        "name": "analyze_content",
        "category": ToolCategory.CONTENT_ANALYSIS,
        "description": "åˆ†æè§†é¢‘å†…å®¹",
        "params": {"video_url": "string", "analysis_type": "string"},
        "required": ["video_url"]
    },
    {
        "name": "generate_dubbing",
        "category": ToolCategory.AUDIO_DUBBING,
        "description": "ç”Ÿæˆé…éŸ³",
        "params": {"video_url": "string", "target_language": "string", "voice_id": "string"},
        "required": ["video_url", "target_language"]
    },
    {
        "name": "export_project",
        "category": ToolCategory.FILE_MANAGEMENT,
        "description": "å¯¼å‡ºé¡¹ç›®",
        "params": {"project_id": "string", "format": "string", "quality": "string"},
        "required": ["project_id"]
    },
    {
        "name": "schedule_task",
        "category": ToolCategory.SCHEDULING,
        "description": "å®‰æ’å®šæ—¶ä»»åŠ¡",
        "params": {"task_type": "string", "scheduled_time": "string", "video_url": "string"},
        "required": ["task_type", "scheduled_time"]
    },
    {
        "name": "extract_keyframes",
        "category": ToolCategory.VIDEO_PROCESSING,
        "description": "æå–å…³é”®å¸§",
        "params": {"video_url": "string", "interval": "number", "max_frames": "number"},
        "required": ["video_url"]
    },
]

# æŸ¥è¯¢æ¨¡æ¿
QUERY_TEMPLATES = {
    "parse_video": [
        "å¸®æˆ‘åˆ†æè¿™ä¸ªè§†é¢‘ï¼š{video_url}",
        "è§£æè§†é¢‘ {video_url}",
        "æˆ‘éœ€è¦è§£æä¸€ä¸‹ {video_url} è¿™ä¸ªè§†é¢‘çš„ä¿¡æ¯",
        "Parse this video: {video_url}",
        "åˆ†æè§†é¢‘ {video_url}ï¼Œæˆ‘æƒ³çŸ¥é“å®ƒçš„æ—¶é•¿å’Œåˆ†è¾¨ç‡",
    ],
    "generate_subtitles": [
        "ç»™è§†é¢‘ {video_url} ç”Ÿæˆ{language}å­—å¹•",
        "ä¸º {video_url} åˆ›å»ºå­—å¹•",
        "å¸®æˆ‘æŠŠ {video_url} çš„è¯­éŸ³è½¬æˆå­—å¹•",
        "Generate subtitles for {video_url}",
        "è¯·ä¸ºè¿™ä¸ªè§†é¢‘æ·»åŠ {language}å­—å¹•ï¼š{video_url}",
    ],
    "translate_subtitles": [
        "æŠŠå­—å¹•æ–‡ä»¶ {subtitle_file} ç¿»è¯‘æˆ{target_lang}",
        "ç¿»è¯‘å­—å¹• {subtitle_file} åˆ°{target_lang}",
        "Translate {subtitle_file} to {target_lang}",
        "æˆ‘éœ€è¦æŠŠå­—å¹•ç¿»è¯‘æˆ{target_lang}",
    ],
    "analyze_content": [
        "åˆ†æä¸€ä¸‹è§†é¢‘ {video_url} çš„å†…å®¹",
        "å¸®æˆ‘æ€»ç»“ {video_url} è¿™ä¸ªè§†é¢‘è®²äº†ä»€ä¹ˆ",
        "æå–è§†é¢‘ {video_url} çš„ä¸»é¢˜å’Œå…³é”®ç‚¹",
        "Analyze the content of {video_url}",
    ],
    "generate_dubbing": [
        "ç»™è§†é¢‘ {video_url} é…ä¸Š{language}é…éŸ³",
        "ç”¨AIå£°éŸ³ä¸º {video_url} ç”Ÿæˆ{language}é…éŸ³",
        "æˆ‘æƒ³ç»™è¿™ä¸ªè§†é¢‘æ·»åŠ {language}è¯­éŸ³ï¼š{video_url}",
        "Generate {language} dubbing for {video_url}",
    ],
    "export_project": [
        "å¯¼å‡ºé¡¹ç›® {project_id}",
        "æŠŠé¡¹ç›® {project_id} å¯¼å‡ºä¸º{format}æ ¼å¼",
        "Export project {project_id} as {format}",
    ],
    "schedule_task": [
        "å®‰æ’åœ¨{time}å¤„ç†è§†é¢‘ {video_url}",
        "å®šæ—¶ä»»åŠ¡ï¼š{time}æ‰§è¡Œå­—å¹•ç”Ÿæˆ",
        "Schedule video processing for {time}",
    ],
    "extract_keyframes": [
        "æå–è§†é¢‘ {video_url} çš„å…³é”®å¸§",
        "ä» {video_url} ä¸­æå–å…³é”®ç”»é¢",
        "Extract keyframes from {video_url}",
    ],
}

# æ ·æœ¬æ•°æ®
SAMPLE_DATA = {
    "video_url": [
        "https://example.com/video1.mp4",
        "https://storage.example.com/uploads/meeting_2024.mp4",
        "/data/videos/tutorial.mp4",
        "https://cdn.example.com/content/lecture_01.mp4",
        "https://media.example.org/clip.mp4",
        "/videos/presentation.mp4",
    ],
    "language": ["ä¸­æ–‡", "è‹±æ–‡", "æ—¥æ–‡", "éŸ©æ–‡", "Chinese", "English", "Japanese"],
    "target_lang": ["ä¸­æ–‡", "è‹±æ–‡", "æ—¥æ–‡", "éŸ©æ–‡", "Chinese", "English", "Japanese", "Korean"],
    "subtitle_file": [
        "/subtitles/video1.srt",
        "subtitles/meeting.vtt",
        "/data/subs/lecture.srt",
        "/output/captions.srt",
    ],
    "time": ["æ˜å¤©ä¸Šåˆ10ç‚¹", "2024-03-15 14:00", "ä¸‹å‘¨ä¸€", "tonight at 8pm", "3å°æ—¶å"],
    "project_id": ["proj_001", "proj_abc123", "video_project_2024", "my_project"],
    "format": ["mp4", "webm", "mov", "avi"],
}


def generate_training_example(tool: dict) -> dict:
    """ç”Ÿæˆå•ä¸ªè®­ç»ƒæ ·æœ¬"""
    tool_name = tool["name"]
    
    # é€‰æ‹©æŸ¥è¯¢æ¨¡æ¿
    templates = QUERY_TEMPLATES.get(tool_name, [f"ä½¿ç”¨ {tool_name}"])
    template = random.choice(templates)
    
    # å¡«å……æ¨¡æ¿
    query = template
    for key, values in SAMPLE_DATA.items():
        placeholder = "{" + key + "}"
        if placeholder in query:
            query = query.replace(placeholder, random.choice(values))
    
    # ç”Ÿæˆå‚æ•°
    arguments = {}
    for param_name in tool["params"]:
        if param_name == "video_url":
            arguments[param_name] = random.choice(SAMPLE_DATA["video_url"])
        elif param_name == "subtitle_file":
            arguments[param_name] = random.choice(SAMPLE_DATA["subtitle_file"])
        elif param_name in ["source_language", "target_language"]:
            arguments[param_name] = random.choice(["zh", "en", "ja", "ko"])
        elif param_name == "language":
            arguments[param_name] = random.choice(SAMPLE_DATA["language"])
        elif param_name == "analysis_type":
            arguments[param_name] = random.choice(["summary", "topics", "all"])
        elif param_name == "voice_id":
            arguments[param_name] = random.choice(["voice_001", "voice_002", "default"])
        elif param_name == "format":
            arguments[param_name] = random.choice(SAMPLE_DATA["format"])
        elif param_name == "quality":
            arguments[param_name] = random.choice(["720p", "1080p", "4k"])
        elif param_name == "project_id":
            arguments[param_name] = random.choice(SAMPLE_DATA["project_id"])
        elif param_name == "task_type":
            arguments[param_name] = random.choice(["subtitle", "dubbing", "analysis"])
        elif param_name == "scheduled_time":
            arguments[param_name] = random.choice(SAMPLE_DATA["time"])
        elif param_name == "interval":
            arguments[param_name] = random.choice([1, 5, 10, 30])
        elif param_name == "max_frames":
            arguments[param_name] = random.choice([10, 20, 50, 100])
        else:
            arguments[param_name] = f"value_{param_name}"
    
    # æ„å»ºè®­ç»ƒæ ¼å¼
    tool_call = {"name": tool_name, "arguments": arguments}
    
    return {
        "messages": [
            {
                "role": "system",
                "content": "You are a helpful AI assistant. Respond with JSON tool calls when appropriate."
            },
            {
                "role": "user",
                "content": query
            },
            {
                "role": "assistant",
                "content": json.dumps(tool_call, ensure_ascii=False)
            }
        ]
    }


def generate_dataset(num_samples: int, output_path: str) -> str:
    """ç”Ÿæˆè®­ç»ƒæ•°æ®é›†"""
    print(f"æ­£åœ¨ç”Ÿæˆ {num_samples} æ¡è®­ç»ƒæ•°æ®...")
    
    examples = []
    for i in range(num_samples):
        tool = random.choice(TOOLS)
        example = generate_training_example(tool)
        examples.append(example)
        
        if (i + 1) % 100 == 0:
            print(f"  å·²ç”Ÿæˆ {i + 1}/{num_samples} æ¡")
    
    # ä¿å­˜
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, "w", encoding="utf-8") as f:
        for example in examples:
            f.write(json.dumps(example, ensure_ascii=False) + "\n")
    
    print(f"æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
    return str(output_path)


# ============================================================================
# è®­ç»ƒéƒ¨åˆ†
# ============================================================================

def train_model(
    data_path: str,
    output_dir: str = "outputs/tool_use_model",
    num_epochs: int = 3,
    batch_size: int = 2,
    learning_rate: float = 2e-4,
    lora_r: int = 64,
):
    """ä½¿ç”¨ GPU è®­ç»ƒæ¨¡å‹"""
    print("\n" + "=" * 60)
    print("å¼€å§‹è®­ç»ƒæ¨¡å‹")
    print("=" * 60)
    
    # Windows ä¸Š Unsloth æœ‰å…¼å®¹æ€§é—®é¢˜ï¼Œç›´æ¥ä½¿ç”¨æ ‡å‡†è®­ç»ƒ
    import platform
    if platform.system() == "Windows":
        print("Windows ç³»ç»Ÿï¼Œä½¿ç”¨æ ‡å‡†è®­ç»ƒæ¨¡å¼")
        return train_standard(data_path, output_dir, num_epochs, batch_size, learning_rate, lora_r)
    
    try:
        # å°è¯•ä½¿ç”¨ unslothï¼ˆæ›´å¿«ï¼Œä»… Linuxï¼‰
        from unsloth import FastLanguageModel
        print("ä½¿ç”¨ Unsloth åŠ é€Ÿè®­ç»ƒ")
        use_unsloth = True
    except ImportError:
        print("Unsloth ä¸å¯ç”¨ï¼Œä½¿ç”¨æ ‡å‡†è®­ç»ƒ")
        use_unsloth = False
    
    if use_unsloth:
        return train_with_unsloth(data_path, output_dir, num_epochs, batch_size, learning_rate, lora_r)
    else:
        return train_standard(data_path, output_dir, num_epochs, batch_size, learning_rate, lora_r)


def train_with_unsloth(
    data_path: str,
    output_dir: str,
    num_epochs: int,
    batch_size: int,
    learning_rate: float,
    lora_r: int = 64,
):
    """ä½¿ç”¨ Unsloth è®­ç»ƒ"""
    from unsloth import FastLanguageModel
    from trl import SFTTrainer, SFTConfig
    from datasets import load_dataset
    
    print(f"åŠ è½½æ¨¡å‹: unsloth/Qwen2.5-3B-Instruct-bnb-4bit")
    
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name="unsloth/Qwen2.5-3B-Instruct-bnb-4bit",
        max_seq_length=2048,
        dtype=None,
        load_in_4bit=True,
    )
    
    # æ·»åŠ  LoRA
    model = FastLanguageModel.get_peft_model(
        model,
        r=lora_r,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                       "gate_proj", "up_proj", "down_proj"],
        lora_alpha=lora_r * 2,  # alpha = 2 * r
        lora_dropout=0.05,
        bias="none",
        use_gradient_checkpointing="unsloth",
    )
    
    # åŠ è½½æ•°æ®
    print(f"åŠ è½½æ•°æ®: {data_path}")
    dataset = load_dataset("json", data_files=data_path, split="train")
    
    def format_example(example):
        messages = example["messages"]
        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
        return {"text": text}
    
    dataset = dataset.map(format_example)
    
    # è®­ç»ƒé…ç½® (æ–°ç‰ˆ trl ä½¿ç”¨ SFTConfig)
    sft_config = SFTConfig(
        output_dir=output_dir,
        num_train_epochs=num_epochs,
        per_device_train_batch_size=batch_size,
        gradient_accumulation_steps=4,
        learning_rate=learning_rate,
        warmup_ratio=0.1,
        logging_steps=10,
        save_steps=100,
        save_total_limit=2,
        fp16=True,
        optim="adamw_8bit",
        seed=42,
        max_seq_length=2048,
        dataset_text_field="text",
    )
    
    trainer = SFTTrainer(
        model=model,
        train_dataset=dataset,
        processing_class=tokenizer,
        args=sft_config,
    )
    
    print("\nå¼€å§‹è®­ç»ƒ...")
    trainer.train()
    
    # ä¿å­˜
    print(f"\nä¿å­˜æ¨¡å‹åˆ°: {output_dir}")
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    # å¯¼å‡ºåˆå¹¶æ¨¡å‹
    merged_dir = f"{output_dir}_merged"
    print(f"å¯¼å‡ºåˆå¹¶æ¨¡å‹åˆ°: {merged_dir}")
    model.save_pretrained_merged(merged_dir, tokenizer, save_method="merged_16bit")
    
    print("\nè®­ç»ƒå®Œæˆ!")
    return output_dir


def train_standard(
    data_path: str,
    output_dir: str,
    num_epochs: int,
    batch_size: int,
    learning_rate: float,
    lora_r: int = 64,
):
    """
    æ ‡å‡† transformers è®­ç»ƒ - å±•ç¤ºå®Œæ•´çš„å¾®è°ƒæŠ€æœ¯æ ˆ
    
    æŠ€æœ¯äº®ç‚¹:
    1. QLoRA é‡åŒ–å¾®è°ƒ - 4bit é‡åŒ– + LoRAï¼Œæ˜¾å­˜é™ä½ 75%
    2. è‡ªå®šä¹‰æŸå¤±å‡½æ•° - é’ˆå¯¹å·¥å…·è°ƒç”¨ä»»åŠ¡ä¼˜åŒ–
    3. å­¦ä¹ ç‡è°ƒåº¦ - Cosine with warmup
    4. æ¢¯åº¦è£å‰ª - é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
    5. æ—©åœæœºåˆ¶ - é˜²æ­¢è¿‡æ‹Ÿåˆ
    6. è®­ç»ƒç›‘æ§ - å®æ—¶ loss æ›²çº¿
    """
    import torch
    import torch.nn as nn
    from transformers import (
        AutoModelForCausalLM,
        AutoTokenizer,
        BitsAndBytesConfig,
        TrainingArguments,
        Trainer,
        DataCollatorForLanguageModeling,
        EarlyStoppingCallback,
    )
    from transformers.trainer_callback import TrainerCallback
    from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
    from datasets import load_dataset
    import math
    
    print("=" * 60)
    print("æ¨¡å‹å¾®è°ƒ - æŠ€æœ¯é…ç½®è¯¦æƒ…")
    print("=" * 60)
    
    # ========================================
    # 1. é‡åŒ–é…ç½® (QLoRA)
    # ========================================
    print("\n[1] é‡åŒ–é…ç½® (QLoRA)")
    print("-" * 40)
    
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,              # 4bit é‡åŒ–: 3B æ¨¡å‹ 12GB â†’ 3GB
        bnb_4bit_quant_type="nf4",      # NormalFloat4: æ¯” FP4 æ›´é€‚åˆæ­£æ€åˆ†å¸ƒæƒé‡
        bnb_4bit_compute_dtype=torch.float16,  # è®¡ç®—ç²¾åº¦: FP16 å¹³è¡¡é€Ÿåº¦å’Œç²¾åº¦
        bnb_4bit_use_double_quant=True, # åŒé‡é‡åŒ–: é‡åŒ–å¸¸æ•°ä¹Ÿé‡åŒ–ï¼Œå†çœ 0.4GB
    )
    
    print(f"  - é‡åŒ–ç±»å‹: NF4 (NormalFloat4)")
    print(f"  - è®¡ç®—ç²¾åº¦: FP16")
    print(f"  - åŒé‡é‡åŒ–: å¯ç”¨")
    print(f"  - é¢„è®¡æ˜¾å­˜: ~4GB (åŸå§‹ ~12GB)")
    
    # ========================================
    # 2. æ¨¡å‹åŠ è½½
    # ========================================
    print("\n[2] åŠ è½½åŸºç¡€æ¨¡å‹")
    print("-" * 40)
    print(f"  - æ¨¡å‹: Qwen/Qwen2.5-3B-Instruct")
    print(f"  - å‚æ•°é‡: 3B")
    
    model = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen2.5-3B-Instruct",
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        attn_implementation="eager",  # æˆ– "flash_attention_2" å¦‚æœæ”¯æŒ
    )
    
    tokenizer = AutoTokenizer.from_pretrained(
        "Qwen/Qwen2.5-3B-Instruct",
        trust_remote_code=True,
    )
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"  # å› æœè¯­è¨€æ¨¡å‹ç”¨å³å¡«å……
    
    # ========================================
    # 3. LoRA é…ç½® (å‚æ•°é«˜æ•ˆå¾®è°ƒ)
    # ========================================
    print("\n[3] LoRA é…ç½® (å‚æ•°é«˜æ•ˆå¾®è°ƒ)")
    print("-" * 40)
    
    model = prepare_model_for_kbit_training(
        model,
        use_gradient_checkpointing=True,  # æ¢¯åº¦æ£€æŸ¥ç‚¹: ç”¨è®¡ç®—æ¢æ˜¾å­˜
    )
    
    # LoRA è¶…å‚æ•°è®¾è®¡åŸç†:
    # - r (ç§©): æ§åˆ¶ä½ç§©çŸ©é˜µçš„ç»´åº¦ï¼Œè¶Šå¤§è¡¨è¾¾èƒ½åŠ›è¶Šå¼ºä½†å‚æ•°è¶Šå¤š
    # - alpha: ç¼©æ”¾å› å­ï¼Œalpha/r å†³å®š LoRA æƒé‡çš„å½±å“ç¨‹åº¦
    # - target_modules: é€‰æ‹©å¾®è°ƒå“ªäº›å±‚ï¼Œæ³¨æ„åŠ›å±‚æ•ˆæœæœ€å¥½
    # ä½¿ç”¨å‘½ä»¤è¡Œä¼ å…¥çš„å‚æ•°
    LORA_R = lora_r
    LORA_ALPHA = lora_r * 2  # alpha = 2 * r
    
    lora_config = LoraConfig(
        r=LORA_R,                # ç§©: ç”±å‘½ä»¤è¡Œå‚æ•°æ§åˆ¶
        lora_alpha=LORA_ALPHA,   # alpha = 2 * r
        target_modules=[         # å¾®è°ƒæ‰€æœ‰çº¿æ€§å±‚
            "q_proj", "k_proj", "v_proj", "o_proj",  # æ³¨æ„åŠ›å±‚
            "gate_proj", "up_proj", "down_proj",      # FFN å±‚
        ],
        lora_dropout=0.05,       # Dropout: è½»å¾®æ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ
        bias="none",             # ä¸è®­ç»ƒåç½®: å‡å°‘å‚æ•°
        task_type="CAUSAL_LM",   # ä»»åŠ¡ç±»å‹: å› æœè¯­è¨€æ¨¡å‹
    )
    
    model = get_peft_model(model, lora_config)
    
    # æ‰“å°å‚æ•°ç»Ÿè®¡
    trainable_params, all_params = model.get_nb_trainable_parameters()
    trainable_percent = 100 * trainable_params / all_params
    
    print(f"  - LoRA ç§© (r): {LORA_R}")
    print(f"  - ç¼©æ”¾å› å­ (alpha/r): {LORA_ALPHA / LORA_R}")
    print(f"  - Dropout: {lora_config.lora_dropout}")
    print(f"  - å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({trainable_percent:.2f}%)")
    print(f"  - æ€»å‚æ•°: {all_params:,}")
    
    # ========================================
    # 4. æ•°æ®å¤„ç†
    # ========================================
    print(f"\n[4] æ•°æ®å¤„ç†")
    print("-" * 40)
    print(f"  - æ•°æ®è·¯å¾„: {data_path}")
    
    dataset = load_dataset("json", data_files=data_path, split="train")
    print(f"  - æ ·æœ¬æ•°é‡: {len(dataset)}")
    
    def format_and_tokenize(example):
        """å°†å¯¹è¯æ ¼å¼è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥"""
        messages = example["messages"]
        text = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=False
        )
        tokenized = tokenizer(
            text,
            truncation=True,
            max_length=2048,
            padding="max_length",
        )
        # è®¾ç½® labels: -100 è¡¨ç¤ºä¸è®¡ç®—æŸå¤±çš„ä½ç½®
        tokenized["labels"] = tokenized["input_ids"].copy()
        return tokenized
    
    dataset = dataset.map(
        format_and_tokenize, 
        remove_columns=dataset.column_names,
        desc="Tokenizing"
    )
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    dataset = dataset.train_test_split(test_size=0.1, seed=42)
    train_dataset = dataset["train"]
    eval_dataset = dataset["test"]
    
    print(f"  - è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"  - éªŒè¯é›†: {len(eval_dataset)} æ ·æœ¬")
    print(f"  - æœ€å¤§é•¿åº¦: 2048 tokens")
    
    # ========================================
    # 5. è®­ç»ƒè¶…å‚æ•°
    # ========================================
    print(f"\n[5] è®­ç»ƒè¶…å‚æ•°")
    print("-" * 40)
    
    # è®¡ç®—è®­ç»ƒæ­¥æ•°
    effective_batch_size = batch_size * 4  # gradient_accumulation_steps=4
    steps_per_epoch = math.ceil(len(train_dataset) / effective_batch_size)
    total_steps = steps_per_epoch * num_epochs
    warmup_steps = int(total_steps * 0.1)
    
    print(f"  - å­¦ä¹ ç‡: {learning_rate}")
    print(f"  - æ‰¹æ¬¡å¤§å°: {batch_size} (æœ‰æ•ˆ: {effective_batch_size})")
    print(f"  - è®­ç»ƒè½®æ•°: {num_epochs}")
    print(f"  - æ€»æ­¥æ•°: {total_steps}")
    print(f"  - é¢„çƒ­æ­¥æ•°: {warmup_steps}")
    print(f"  - ä¼˜åŒ–å™¨: AdamW 8bit (æ˜¾å­˜ä¼˜åŒ–)")
    print(f"  - å­¦ä¹ ç‡è°ƒåº¦: Cosine")
    print(f"  - æ¢¯åº¦è£å‰ª: 1.0")
    
    training_args = TrainingArguments(
        output_dir=output_dir,
        
        # è®­ç»ƒè½®æ•°å’Œæ‰¹æ¬¡
        num_train_epochs=num_epochs,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        gradient_accumulation_steps=4,      # æ¢¯åº¦ç´¯ç§¯: æ¨¡æ‹Ÿæ›´å¤§æ‰¹æ¬¡
        
        # å­¦ä¹ ç‡é…ç½®
        learning_rate=learning_rate,
        lr_scheduler_type="cosine",         # Cosine é€€ç«: å¹³æ»‘é™ä½å­¦ä¹ ç‡
        warmup_ratio=0.1,                   # 10% é¢„çƒ­: ç¨³å®šåˆå§‹è®­ç»ƒ
        
        # ä¼˜åŒ–å™¨
        optim="paged_adamw_8bit",           # 8bit AdamW: æ˜¾å­˜å ç”¨å‡åŠ
        weight_decay=0.01,                  # L2 æ­£åˆ™åŒ–: é˜²æ­¢è¿‡æ‹Ÿåˆ
        max_grad_norm=1.0,                  # æ¢¯åº¦è£å‰ª: é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        
        # ç²¾åº¦
        fp16=True,                          # æ··åˆç²¾åº¦è®­ç»ƒ: é€Ÿåº¦ç¿»å€
        
        # æ—¥å¿—å’Œä¿å­˜
        logging_steps=10,
        eval_strategy="steps",              # æŒ‰æ­¥æ•°è¯„ä¼°
        eval_steps=50,                      # æ¯ 50 æ­¥è¯„ä¼°ä¸€æ¬¡
        save_strategy="steps",
        save_steps=100,
        save_total_limit=2,
        load_best_model_at_end=True,        # è®­ç»ƒç»“æŸåŠ è½½æœ€ä½³æ¨¡å‹
        metric_for_best_model="eval_loss",  # ç”¨éªŒè¯æŸå¤±é€‰æ‹©æœ€ä½³æ¨¡å‹
        greater_is_better=False,
        
        # å…¶ä»–
        seed=42,
        remove_unused_columns=False,
        report_to="none",                   # ç¦ç”¨ wandb ç­‰
    )
    
    # ========================================
    # 6. è‡ªå®šä¹‰å›è°ƒ (è®­ç»ƒç›‘æ§)
    # ========================================
    class TrainingMonitorCallback(TrainerCallback):
        """è®­ç»ƒç›‘æ§å›è°ƒ - å®æ—¶æ˜¾ç¤ºè®­ç»ƒçŠ¶æ€"""
        
        def __init__(self):
            self.train_losses = []
            self.eval_losses = []
            
        def on_log(self, args, state, control, logs=None, **kwargs):
            if logs:
                if "loss" in logs:
                    self.train_losses.append(logs["loss"])
                if "eval_loss" in logs:
                    self.eval_losses.append(logs["eval_loss"])
                    
        def on_evaluate(self, args, state, control, metrics=None, **kwargs):
            if metrics:
                eval_loss = metrics.get("eval_loss", 0)
                print(f"\n  ğŸ“Š éªŒè¯æŸå¤±: {eval_loss:.4f}")
                if self.train_losses:
                    print(f"  ğŸ“ˆ è®­ç»ƒæŸå¤±: {self.train_losses[-1]:.4f}")
    
    # æ•°æ®æ•´ç†å™¨
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,  # å› æœè¯­è¨€æ¨¡å‹ï¼Œä¸æ˜¯ MLM
    )
    
    # ========================================
    # 7. åˆ›å»º Trainer
    # ========================================
    print(f"\n[6] æŸå¤±å‡½æ•°")
    print("-" * 40)
    print(f"  - ç±»å‹: CrossEntropyLoss (è¯­è¨€æ¨¡å‹æ ‡å‡†æŸå¤±)")
    print(f"  - å¿½ç•¥ç´¢å¼•: -100 (padding tokens)")
    print(f"  - æ ‡ç­¾å¹³æ»‘: æ—  (ä¿æŒè¾“å‡ºåˆ†å¸ƒé”åˆ©)")
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
        callbacks=[
            TrainingMonitorCallback(),
            EarlyStoppingCallback(
                early_stopping_patience=3,  # 3 æ¬¡è¯„ä¼°æ— æ”¹å–„åˆ™åœæ­¢
                early_stopping_threshold=0.01,
            ),
        ],
    )
    
    # ========================================
    # 8. å¼€å§‹è®­ç»ƒ
    # ========================================
    print(f"\n" + "=" * 60)
    print("å¼€å§‹è®­ç»ƒ")
    print("=" * 60)
    
    train_result = trainer.train()
    
    # ========================================
    # 9. è®­ç»ƒç»“æœ
    # ========================================
    print(f"\n" + "=" * 60)
    print("è®­ç»ƒå®Œæˆ - ç»“æœæ±‡æ€»")
    print("=" * 60)
    
    metrics = train_result.metrics
    print(f"\n  è®­ç»ƒæŸå¤±: {metrics.get('train_loss', 'N/A'):.4f}")
    print(f"  è®­ç»ƒæ­¥æ•°: {metrics.get('train_steps', 'N/A')}")
    print(f"  è®­ç»ƒæ—¶é—´: {metrics.get('train_runtime', 0):.1f} ç§’")
    
    # æœ€ç»ˆè¯„ä¼°
    eval_metrics = trainer.evaluate()
    print(f"  éªŒè¯æŸå¤±: {eval_metrics.get('eval_loss', 'N/A'):.4f}")
    
    # ä¿å­˜
    print(f"\nä¿å­˜æ¨¡å‹åˆ°: {output_dir}")
    trainer.save_model(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    # ä¿å­˜è®­ç»ƒé…ç½®
    import json
    config_path = Path(output_dir) / "training_config.json"
    with open(config_path, "w", encoding="utf-8") as f:
        json.dump({
            "model": "Qwen/Qwen2.5-3B-Instruct",
            "quantization": "4bit NF4",
            "lora_r": lora_config.r,
            "lora_alpha": lora_config.lora_alpha,
            "learning_rate": learning_rate,
            "epochs": num_epochs,
            "batch_size": batch_size,
            "train_samples": len(train_dataset),
            "final_train_loss": metrics.get('train_loss'),
            "final_eval_loss": eval_metrics.get('eval_loss'),
        }, f, indent=2)
    
    print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
    return output_dir


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description="ç”Ÿæˆæ•°æ®å¹¶è®­ç»ƒæ¨¡å‹")
    parser.add_argument("--samples", type=int, default=500, help="ç”Ÿæˆæ ·æœ¬æ•°é‡")
    parser.add_argument("--epochs", type=int, default=3, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch-size", type=int, default=2, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--lr", type=float, default=2e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--lora-r", type=int, default=64, help="LoRA ç§© (æ¨è: 32/64/128)")
    parser.add_argument("--output-dir", type=str, default="outputs/tool_model", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--data-only", action="store_true", help="åªç”Ÿæˆæ•°æ®ï¼Œä¸è®­ç»ƒ")
    parser.add_argument("--train-only", action="store_true", help="åªè®­ç»ƒï¼Œä½¿ç”¨å·²æœ‰æ•°æ®")
    parser.add_argument("--data-path", type=str, default="data/generated_train.jsonl", help="æ•°æ®è·¯å¾„")
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("Edge SLM è®­ç»ƒå·¥å…·")
    print("=" * 60)
    print(f"æ ·æœ¬æ•°é‡: {args.samples}")
    print(f"è®­ç»ƒè½®æ•°: {args.epochs}")
    print(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"å­¦ä¹ ç‡: {args.lr}")
    print(f"LoRA ç§©: {args.lora_r}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print("=" * 60)
    
    data_path = args.data_path
    
    # ç”Ÿæˆæ•°æ®
    if not args.train_only:
        data_path = generate_dataset(args.samples, args.data_path)
    
    # è®­ç»ƒ
    if not args.data_only:
        train_model(
            data_path=data_path,
            output_dir=args.output_dir,
            num_epochs=args.epochs,
            batch_size=args.batch_size,
            learning_rate=args.lr,
            lora_r=args.lora_r,
        )
    
    print("\n" + "=" * 60)
    print("å®Œæˆ!")
    print("=" * 60)


if __name__ == "__main__":
    main()
